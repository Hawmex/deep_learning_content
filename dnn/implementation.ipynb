{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\n",
    "# Implementing a Deep Neural Network\n",
    "\n",
    "By Hamed Araab\n",
    "\n",
    "Supervisor: Dr. Marzieh Zarinbal\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates a basic implementation of deep neural networks (DNNs)\n",
    "based on the specifications of `reference.pdf`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "\n",
    "First of all, let's import the required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network\n",
    "\n",
    "In this section, we implement two main classes:\n",
    "\n",
    "- `FullyConnectedLayer`, which will be used to declare a layer in the network,\n",
    "  and\n",
    "- `NeuralNetwork`, which will be used to declare the network itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, units, activation_function):\n",
    "        self.units = units\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    @staticmethod\n",
    "    def get_initial_params(shape, low=-1, high=1):\n",
    "        return np.random.rand(*shape) * (high - low) + low\n",
    "\n",
    "    def __init__(self, input_size, layers, loss_function, learning_rate):\n",
    "        self.input_size = input_size\n",
    "        self.number_of_layers = len(layers)\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.layers = {l + 1: layers[l] for l in range(self.number_of_layers)}\n",
    "\n",
    "        self.weights = {\n",
    "            1: NeuralNetwork.get_initial_params(\n",
    "                (self.layers[1].units, self.input_size)\n",
    "            ),\n",
    "        } | {\n",
    "            l: NeuralNetwork.get_initial_params(\n",
    "                (self.layers[l].units, self.layers[l - 1].units),\n",
    "            )\n",
    "            for l in range(2, self.number_of_layers + 1)\n",
    "        }\n",
    "\n",
    "        self.biases = {\n",
    "            l: NeuralNetwork.get_initial_params((self.layers[l].units, 1))\n",
    "            for l in range(1, self.number_of_layers + 1)\n",
    "        }\n",
    "\n",
    "    # Forward Propagation\n",
    "    def predict(self, X, return_objects=False):\n",
    "        Z = {}\n",
    "        A = {0: X}\n",
    "\n",
    "        for l in range(1, self.number_of_layers + 1):\n",
    "            Z[l] = self.weights[l] @ A[l - 1] + self.biases[l]\n",
    "            A[l] = self.layers[l].activation_function.get_activation(Z[l])\n",
    "\n",
    "        if return_objects:\n",
    "            return Z, A\n",
    "        else:\n",
    "            Y_hat = A[self.number_of_layers]\n",
    "\n",
    "            return Y_hat\n",
    "\n",
    "    # Training Phase (Forward and Backward Propagation)\n",
    "    def train(self, X, Y, X_test=None, Y_test=None, epochs=10, batch_size=32):\n",
    "        _, m = X.shape\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data for each epoch\n",
    "            permutation = np.random.permutation(m)\n",
    "\n",
    "            X_shuffled = X[:, permutation]\n",
    "            Y_shuffled = Y[:, permutation]\n",
    "\n",
    "            batches = math.ceil(m / batch_size)\n",
    "\n",
    "            cost_epoch = 0\n",
    "\n",
    "            for batch in range(batches):\n",
    "                start_index = batch * batch_size\n",
    "                end_index = start_index + batch_size\n",
    "\n",
    "                X_batch = X_shuffled[:, start_index:end_index]\n",
    "                Y_batch = Y_shuffled[:, start_index:end_index]\n",
    "\n",
    "                Z_batch, A_batch = self.predict(X_batch, return_objects=True)\n",
    "\n",
    "                Y_hat_batch = A_batch[self.number_of_layers]\n",
    "\n",
    "                loss_batch = self.loss_function.get_loss(Y_batch, Y_hat_batch)\n",
    "                cost_epoch += loss_batch / m\n",
    "\n",
    "                self.update_parameters(Y_batch, Z_batch, A_batch)\n",
    "\n",
    "            details = (\n",
    "                f\"Epoch: {epoch + 1}/{epochs}, cost: {'{:.3f}'.format(cost_epoch)}\"\n",
    "            )\n",
    "\n",
    "            if X_test is not None and Y_test is not None:\n",
    "                _, m_test = X_test.shape\n",
    "\n",
    "                Y_hat_test = self.predict(X_test)\n",
    "\n",
    "                cost_test_epoch = (\n",
    "                    self.loss_function.get_loss(Y_test, Y_hat_test) / m_test\n",
    "                )\n",
    "\n",
    "                details += f\", cost_test: {'{:.3f}'.format(cost_test_epoch)}\"\n",
    "\n",
    "            print(details)\n",
    "\n",
    "    # Backward Propagation\n",
    "    def update_parameters(self, Y, Z, A):\n",
    "        _, m = Y.shape\n",
    "\n",
    "        Y_hat = A[self.number_of_layers]\n",
    "\n",
    "        dC_dY_hat = self.loss_function.get_derivative(Y, Y_hat) / m\n",
    "\n",
    "        dC_dA = {self.number_of_layers: dC_dY_hat}\n",
    "\n",
    "        for l in range(self.number_of_layers, 0, -1):\n",
    "            delta = np.tensordot(\n",
    "                dC_dA[l],\n",
    "                self.layers[l].activation_function.get_derivative(Z[l]),\n",
    "            )\n",
    "\n",
    "            dC_dW = delta @ A[l - 1].T\n",
    "            dC_db = delta @ np.ones((m, 1))\n",
    "\n",
    "            self.weights[l] -= self.learning_rate * dC_dW\n",
    "            self.biases[l] -= self.learning_rate * dC_db\n",
    "\n",
    "            if l > 1:\n",
    "                dC_dA[l - 1] = self.weights[l].T @ delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Now, we are going to write down the activation functions based on the following\n",
    "class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def get_activation(self, Z_l):\n",
    "        pass\n",
    "\n",
    "    def get_derivative(self, Z_l):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further on, you are going to see that two versions of `get_derivative` are\n",
    "implemented for each activation function. The first one uses a for-loop and is\n",
    "commented out due to being extremely slow. The second one is quite fast since it\n",
    "uses a vectorized approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(ActivationFunction):\n",
    "    def get_activation(self, Z_l):\n",
    "        return np.maximum(0, Z_l)\n",
    "\n",
    "    def get_derivative(self, Z_l):\n",
    "        # n, m = Z_l.shape\n",
    "\n",
    "        # derivative = np.zeros((n, m, n, m))\n",
    "\n",
    "        # for (i, j, u, v), _ in np.ndenumerate(derivative):\n",
    "        #     if i == u and j == v and Z_l[i, j] > 0:\n",
    "        #         derivative[i, j, u, v] = 1\n",
    "\n",
    "        # return derivative\n",
    "\n",
    "        n, m = Z_l.shape\n",
    "        i, j = np.indices((n, m))\n",
    "\n",
    "        Z_l[Z_l > 0] = 1\n",
    "        Z_l[Z_l <= 0] = 0\n",
    "\n",
    "        derivative = np.zeros((n, m, n, m))\n",
    "\n",
    "        derivative[i, j, i, j] = Z_l[i, j]\n",
    "\n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def get_activation(self, Z_l):\n",
    "        return 1 / (1 + np.exp(-Z_l))\n",
    "\n",
    "    def get_derivative(self, Z_l):\n",
    "        # n, m = Z_l.shape\n",
    "\n",
    "        # derivative = np.zeros((n, m, n, m))\n",
    "\n",
    "        # A_l = self.get_activation(Z_l)\n",
    "\n",
    "        # for (i, j, u, v), _ in np.ndenumerate(derivative):\n",
    "        #     if i == u and j == v:\n",
    "        #         derivative[i, j, u, v] = A_l[i, j] * (1 - A_l[i, j])\n",
    "\n",
    "        # return derivative\n",
    "\n",
    "        n, m = Z_l.shape\n",
    "        i, j = np.indices((n, m))\n",
    "\n",
    "        derivative = np.zeros((n, m, n, m))\n",
    "\n",
    "        A_l = self.get_activation(Z_l)\n",
    "\n",
    "        derivative[i, j, i, j] = A_l[i, j] * (1 - A_l[i, j])\n",
    "\n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftMax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(ActivationFunction):\n",
    "    def get_activation(self, Z_l):\n",
    "        exp_Z_l = np.exp(Z_l)\n",
    "\n",
    "        return exp_Z_l / np.sum(exp_Z_l, axis=0, keepdims=True)\n",
    "\n",
    "    def get_derivative(self, Z_l):\n",
    "        # n, m = Z_l.shape\n",
    "\n",
    "        # derivative = np.zeros((n, m, n, m))\n",
    "\n",
    "        # A_l = self.get_activation(Z_l)\n",
    "\n",
    "        # for (i, j, u, v), _ in np.ndenumerate(derivative):\n",
    "        #     if j == v:\n",
    "        #         if i == u:\n",
    "        #             derivative[i, j, u, v] = A_l[i, j] * (1 - A_l[i, j])\n",
    "        #         else:\n",
    "        #             derivative[i, j, u, v] = -A_l[i, j] * A_l[u, j]\n",
    "\n",
    "        # return derivative\n",
    "\n",
    "        n, m = Z_l.shape\n",
    "        i, j, u = np.indices((n, m, n))\n",
    "\n",
    "        derivative = np.zeros((n, m, n, m))\n",
    "\n",
    "        A_l = self.get_activation(Z_l)\n",
    "\n",
    "        derivative[i, j, u, j] = -A_l[i, j] * A_l[u, j]\n",
    "        derivative[i, j, i, j] += A_l[i, j]\n",
    "\n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Similarly, we will be implementing the loss functions based on this class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def get_loss(self, Y, Y_hat):\n",
    "        pass\n",
    "\n",
    "    def get_derivative(self, Y, Y_hat):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squared Errors (SSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSELoss(LossFunction):\n",
    "    def get_loss(self, Y, Y_hat):\n",
    "        return 1 / 2 * np.sum((Y_hat - Y) ** 2)\n",
    "\n",
    "    def get_derivative(self, Y, Y_hat):\n",
    "        return Y_hat - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy (BCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss(LossFunction):\n",
    "    def get_loss(self, Y, Y_hat):\n",
    "        return -np.sum((Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)))\n",
    "\n",
    "    def get_derivative(self, Y, Y_hat):\n",
    "        return (Y_hat - Y) / (Y_hat * (1 - Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy (CE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELoss(LossFunction):\n",
    "    def get_loss(self, Y, Y_hat):\n",
    "        return -np.sum((Y * np.log(Y_hat)))\n",
    "\n",
    "    def get_derivative(self, Y, Y_hat):\n",
    "        return -Y / Y_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
