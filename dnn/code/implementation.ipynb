{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\n",
    "# Implementing a Deep Neural Network\n",
    "\n",
    "By Hamed Araab\n",
    "\n",
    "Supervisor: Dr. Marzieh Zarinbal\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates a basic implementation of deep neural networks (DNNs)\n",
    "based on the specifications of `../reference.pdf`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "\n",
    "First of all, let's import the required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network (DNN) Implementation\n",
    "\n",
    "In this section, we implement two main classes:\n",
    "\n",
    "- `FullyConnectedLayer`, which will be used to declare a layer in the network,\n",
    "  and\n",
    "- `NeuralNetwork`, which will be used to declare the network itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, units, activation):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def generate_initial_weights(self, l):\n",
    "        prev_units = self.input_size if l == 1 else self.layers[l - 1].units\n",
    "        current_units = self.layers[l].units\n",
    "\n",
    "        return np.random.normal(\n",
    "            0.0,\n",
    "            math.sqrt(2 / current_units),\n",
    "            size=(prev_units, current_units),\n",
    "        )\n",
    "\n",
    "    def __init__(self, input_size, layers, loss, learning_rate):\n",
    "        self.input_size = input_size\n",
    "        self.number_of_layers = len(layers)\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.layers = {l + 1: layers[l] for l in range(self.number_of_layers)}\n",
    "\n",
    "        self.weights = {\n",
    "            l: self.generate_initial_weights(l)\n",
    "            for l in range(1, self.number_of_layers + 1)\n",
    "        }\n",
    "\n",
    "        self.biases = {\n",
    "            l: np.zeros((1, self.layers[l].units))\n",
    "            for l in range(1, self.number_of_layers + 1)\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def trainable_parameters(self):\n",
    "        params = 0\n",
    "\n",
    "        for l in range(1, self.number_of_layers + 1):\n",
    "            prev_units = self.input_size if l == 1 else self.layers[l - 1].units\n",
    "            current_units = self.layers[l].units\n",
    "\n",
    "            # Weights\n",
    "            # params += prev_units * current_units\n",
    "\n",
    "            # Biases\n",
    "            # params += current_units\n",
    "\n",
    "            params += (prev_units + 1) * current_units\n",
    "\n",
    "        return params\n",
    "\n",
    "    def summary(self):\n",
    "        print(f\"Trainable parameters: {self.trainable_parameters}\")\n",
    "\n",
    "    # Forward Propagation\n",
    "    def predict(self, X, return_objects=False):\n",
    "        Z = {}\n",
    "        A = {0: X}\n",
    "\n",
    "        for l in range(1, self.number_of_layers + 1):\n",
    "            Z[l] = A[l - 1] @ self.weights[l] + self.biases[l]\n",
    "            A[l] = self.layers[l].activation.forward(Z[l])\n",
    "\n",
    "        if return_objects:\n",
    "            return Z, A\n",
    "        else:\n",
    "            Y_hat = A[self.number_of_layers]\n",
    "\n",
    "            return Y_hat\n",
    "\n",
    "    # The training phase (forward + backward propagation on training dataset).\n",
    "    def train(self, X, Y, epochs=10, batch_size=32, test_data=None):\n",
    "        n, _ = X.shape\n",
    "\n",
    "        # The total number of mini-batches.\n",
    "        batches = math.ceil(n / batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data for each epoch.\n",
    "            permutation = np.random.permutation(n)\n",
    "\n",
    "            X = X[permutation, :]\n",
    "            Y = Y[permutation, :]\n",
    "\n",
    "            loss_epoch = 0\n",
    "\n",
    "            # Run the training phase for each mini-batch.\n",
    "            for batch in range(batches):\n",
    "                start_index = batch * batch_size\n",
    "                end_index = min(start_index + batch_size, n)\n",
    "\n",
    "                X_batch = X[start_index:end_index, :]\n",
    "                Y_batch = Y[start_index:end_index, :]\n",
    "\n",
    "                Z_batch, A_batch = self.predict(X_batch, return_objects=True)\n",
    "\n",
    "                Y_hat_batch = A_batch[self.number_of_layers]\n",
    "\n",
    "                loss_epoch += self.loss.forward(Y_batch, Y_hat_batch)\n",
    "\n",
    "                self.update_parameters(Y_batch, Z_batch, A_batch)\n",
    "\n",
    "            cost_epoch = loss_epoch / n\n",
    "\n",
    "            details = (\n",
    "                f\"Epoch: {epoch + 1}/{epochs}, cost: {'{:.3f}'.format(cost_epoch)}\"\n",
    "            )\n",
    "\n",
    "            # Calculate the loss for the test split.\n",
    "            if test_data is not None:\n",
    "                X_test, Y_test = test_data\n",
    "\n",
    "                n_test, _ = X_test.shape\n",
    "\n",
    "                Y_hat_test = self.predict(X_test)\n",
    "\n",
    "                loss_test_epoch = self.loss.forward(Y_test, Y_hat_test)\n",
    "                cost_test_epoch = loss_test_epoch / n_test\n",
    "\n",
    "                details += f\", cost_test: {'{:.3f}'.format(cost_test_epoch)}\"\n",
    "\n",
    "            # Print the details containing the prediction cost for this epoch.\n",
    "            print(details)\n",
    "\n",
    "    # Backward propagation.\n",
    "    def update_parameters(self, Y, Z, A):\n",
    "        n, _ = Y.shape\n",
    "\n",
    "        Y_hat = A[self.number_of_layers]\n",
    "\n",
    "        dJ_dY_hat = self.loss.backward(Y, Y_hat)\n",
    "\n",
    "        dC_dA = {self.number_of_layers: dJ_dY_hat / n}\n",
    "\n",
    "        for l in range(self.number_of_layers, 0, -1):\n",
    "            dA_dZ_l = self.layers[l].activation.backward(Z[l])\n",
    "\n",
    "            # Slow, only accepts dA_dZ_l with shape (n, m, n, m).\n",
    "            # delta_l = np.tensordot(dC_dA[l], dA_dZ_l)\n",
    "\n",
    "            # Fast, accepts dA_dZ_l with shapes (n, m, n, m), (n, m, m), and (n, m).\n",
    "            match len(dA_dZ_l.shape):\n",
    "                case 4:\n",
    "                    subscripts = \"ij,ijuv->uv\"\n",
    "                case 3:\n",
    "                    subscripts = \"ij,ijv->iv\"\n",
    "                case 2:\n",
    "                    subscripts = \"ij,ij->ij\"\n",
    "\n",
    "            delta_l = np.einsum(subscripts, dC_dA[l], dA_dZ_l)\n",
    "\n",
    "            dC_dW_l = A[l - 1].T @ delta_l\n",
    "            dC_db_l = np.ones((1, n)) @ delta_l\n",
    "\n",
    "            self.weights[l] -= self.learning_rate * dC_dW_l\n",
    "            self.biases[l] -= self.learning_rate * dC_db_l\n",
    "\n",
    "            if l > 1:\n",
    "                dC_dA[l - 1] = delta_l @ self.weights[l].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Now, we are going to write down the activation functions based on the following\n",
    "class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def forward(self, Z_l):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, Z_l):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further on, you are going to see that two versions of `backward` are implemented\n",
    "for each activation function. The first one uses a for-loop and is commented out\n",
    "due to being extremely slow. The second one is quite fast since it uses a\n",
    "vectorized approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(ActivationFunction):\n",
    "    def forward(self, Z_l):\n",
    "        return np.maximum(0, Z_l)\n",
    "\n",
    "    def backward(self, Z_l):\n",
    "        # Slow, returns an (n, m, n, m) tensor.\n",
    "        # n, m = Z_l.shape\n",
    "        # i, j = np.indices((n, m))\n",
    "\n",
    "        # jacobian = np.zeros((n, m, n, m))\n",
    "\n",
    "        # jacobian[i, j, i, j] = (Z_l[i, j] > 0).astype(int)\n",
    "\n",
    "        # return jacobian\n",
    "        \n",
    "        # Fast, returns an (n, m) matrix.\n",
    "        return (Z_l > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def forward(self, Z_l):\n",
    "        return 1 / (1 + np.exp(-Z_l))\n",
    "\n",
    "    def backward(self, Z_l):\n",
    "        # Slow, returns an (n, m, n, m) tensor.\n",
    "        # n, m = Z_l.shape\n",
    "        # i, j = np.indices((n, m))\n",
    "\n",
    "        # jacobian = np.zeros((n, m, n, m))\n",
    "\n",
    "        # A_l = self.forward(Z_l)\n",
    "\n",
    "        # jacobian[i, j, i, j] = A_l[i, j] * (1 - A_l[i, j])\n",
    "\n",
    "        # return jacobian\n",
    "\n",
    "        # Fast, returns an (n, m) matrix.\n",
    "        A_l = self.forward(Z_l)\n",
    "\n",
    "        return A_l * (1 - A_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(ActivationFunction):\n",
    "    def forward(self, Z_l):\n",
    "        exp_Z_l = np.exp(Z_l)\n",
    "\n",
    "        return exp_Z_l / np.sum(exp_Z_l, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, Z_l):\n",
    "        # Slow, returns an (n, m, n, m) tensor.\n",
    "        # n, m = Z_l.shape\n",
    "        # i, j, v = np.indices((n, m, m))\n",
    "\n",
    "        # jacobian = np.zeros((n, m, n, m))\n",
    "\n",
    "        # A_l = self.forward(Z_l)\n",
    "\n",
    "        # jacobian[i, j, i, v] = A_l[i, j] * ((j == v).astype(int) - A_l[i, v])\n",
    "\n",
    "        # return jacobian\n",
    "\n",
    "        # Fast, returns and (n, m, m) tensor.\n",
    "        n, m = Z_l.shape\n",
    "        i, j, v = np.indices((n, m, m))\n",
    "\n",
    "        derivative = np.zeros((n, m, m))\n",
    "\n",
    "        A_l = self.forward(Z_l)\n",
    "\n",
    "        derivative[i, j, v] = A_l[i, j] * ((j == v).astype(int) - A_l[i, v])\n",
    "\n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Similarly, we will be implementing the loss functions based on this class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def forward(self, Y, Y_hat):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, Y, Y_hat):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squared Errors (SSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSELoss(LossFunction):\n",
    "    def forward(self, Y, Y_hat):\n",
    "        return 1 / 2 * np.sum((Y_hat - Y) ** 2)\n",
    "\n",
    "    def backward(self, Y, Y_hat):\n",
    "        return Y_hat - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy (BCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss(LossFunction):\n",
    "    def forward(self, Y, Y_hat):\n",
    "        # We clip `Y_hat` to avoid overflow since `log(0)` is undefined.\n",
    "        Y_hat = np.clip(Y_hat, epsilon, 1 - epsilon)\n",
    "\n",
    "        return -np.sum((Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)))\n",
    "\n",
    "    def backward(self, Y, Y_hat):\n",
    "        # We clip `Y_hat` to avoid division by zero.\n",
    "        Y_hat = np.clip(Y_hat, epsilon, 1 - epsilon)\n",
    "\n",
    "        return (Y_hat - Y) / (Y_hat * (1 - Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy (CCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCELoss(LossFunction):\n",
    "    def forward(self, Y, Y_hat):\n",
    "        # We clip `Y_hat` to avoid overflow since `log(0)` is undefined.\n",
    "        Y_hat = np.clip(Y_hat, epsilon, None)\n",
    "\n",
    "        return -np.sum((Y * np.log(Y_hat)))\n",
    "\n",
    "    def backward(self, Y, Y_hat):\n",
    "        # We clip `Y_hat` to avoid division by zero.\n",
    "        Y_hat = np.clip(Y_hat, epsilon, None)\n",
    "\n",
    "        return -Y / Y_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
